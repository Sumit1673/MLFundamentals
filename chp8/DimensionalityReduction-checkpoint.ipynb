{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Having large number of features for given problem leads to many problems like slow training, harder to find a solution.\n",
    "\n",
    "    In MNIST image, boarder region of the image is mostly white and is unimportant. So dropping them will not effect the performance of the network. Refer in Chap 7\n",
    "    \n",
    "    Combining two adjacent pixels into a single pixel which are highly corelated will not effect the information of the image.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network should be trained first on the original dataset but if the training is slow you might conider this option.\n",
    "# In some cases, it could be beneficial as it reduces noises and unncessary details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helps in Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**High dimensionality dataset are sparse. The instance are not correlated i.e. they are likely to be far away from each other. Due to this training set has greater risk of getting overfit.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two reduce the effect of overfitting increase the size of the dataset.**\n",
    "\n",
    "**Less features more instances**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DR Technique PROJECTION AND MANIFOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     Projection is projecting the high dimensional data to a subspace of the higher dimension\n",
    "     Projection is not always a good way of dimensionality may suffer from twist and turn like a swiss role. \n",
    "\n",
    "     If suffering from swiss role, better is to unfold the dataset in way to obtain the 2D dataset.\n",
    "\n",
    " <img src='swiss_role.png'><img src='unfolding_swissrole.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANIFOLDING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    A subspace in a high dimension like above where we can represent our dataset and still not loose information\n",
    "    called as MANIFOLD. \n",
    "    \n",
    "    To deal with manifolds is called as Manifold Learning.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Components\n",
    "\n",
    "    PCA identifies the axis that accounts for the largest amount of variance in the train‚Äê\n",
    "    ing set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to the\n",
    "    first one, that accounts for the largest amount of remaining variance.\n",
    "    \n",
    "   **To find the prinicpal components, a matrix factorization method \"Singular Value Decomposition** is used. A matrix X is decomposed into three matrices $$U \\sum_{}^{} V^T$$ where V contains all the principal components.\n",
    "   \n",
    " # PCA centres the data by subtracting it from the mean value before processing. This is an important step before dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
